<!DOCTYPE html>
<html>
<head>
  <title>Call Recorder</title>
  <style>
    body { background: #000; color: #fff; font-family: sans-serif; padding: 20px; }
    #status { margin: 20px 0; }
    .recording { color: #f44336; }
    .connected { color: #4caf50; }
  </style>
</head>
<body>
  <h1>Ghost Recorder</h1>
  <div id="status">Initializing...</div>
  <div id="eventLog" style="max-height: 200px; overflow-y: auto; margin: 10px 0; padding: 10px; background: #111; border-radius: 4px; font-size: 12px;"></div>
  <div id="participants"></div>

  <!-- Hidden video/audio elements for streams -->
  <video id="localVideo" autoplay muted playsinline style="display:none;"></video>
  <div id="remoteVideos" style="display:none;"></div>
  <!-- Hidden audio elements for remote streams - required for Chrome to process WebRTC audio -->
  <div id="remoteAudios" style="display:none;"></div>

  <script>
    // Configuration passed via URL params
    const params = new URLSearchParams(window.location.search);
    const API_URL = params.get('apiUrl') || 'http://localhost:3001';
    const GROUP_ID = params.get('groupId');
    const CALL_ID = params.get('callId');
    const CALL_TYPE = params.get('callType') || 'phone'; // 'phone' or 'video'
    const AUTH_TOKEN = params.get('token');

    // Chunked recording configuration
    // 2-minute chunks with 5-second overlap for gapless recording
    const CHUNK_DURATION_MS = 2 * 60 * 1000; // 2 minutes per chunk
    const OVERLAP_DURATION_MS = 5 * 1000;    // 5 second overlap for gapless recording

    // State
    let peerConnections = {};
    let pendingIceCandidates = {}; // Queue ICE candidates until offer is processed
    let localStream = null;
    let isRecording = false;
    let audioContext = null;
    let mixedDestination = null;

    // Chunked recording state
    let activeRecorders = []; // Array of { mediaRecorder, chunks, chunkIndex, startedAt }
    let currentChunkIndex = 0;
    let chunkTimer = null;
    let uploadQueue = []; // Queue of chunks waiting to be uploaded
    let isUploading = false;

    // Participant tracking
    let participantNames = {}; // { peerId: displayName }

    const statusEl = document.getElementById('status');
    const eventLogEl = document.getElementById('eventLog');

    function updateStatus(msg, className = '') {
      statusEl.textContent = msg;
      statusEl.className = className;
      console.log('[Recorder]', msg);
    }

    // Log events to UI and broadcast to participants
    function logEvent(message, type = 'info') {
      const timestamp = new Date().toLocaleTimeString();
      const entry = document.createElement('div');
      entry.style.color = type === 'warning' ? '#ff9800' : type === 'error' ? '#f44336' : '#4caf50';
      entry.textContent = `[${timestamp}] ${message}`;
      eventLogEl.appendChild(entry);
      eventLogEl.scrollTop = eventLogEl.scrollHeight;
      console.log(`[Recording] ${message}`);
      broadcastRecordingStatus(message);
    }

    // Broadcast recording status to all participants via signaling
    async function broadcastRecordingStatus(message) {
      try {
        const endpoint = CALL_TYPE === 'video' ? 'video-calls' : 'phone-calls';
        await fetch(`${API_URL}/groups/${GROUP_ID}/${endpoint}/${CALL_ID}/recording-status`, {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${AUTH_TOKEN}`,
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({ message })
        });
      } catch (err) {
        // Silently fail - don't disrupt recording for status updates
      }
    }

    async function init() {
      try {
        updateStatus('Getting media access...');

        // Create a silent audio stream using Web Audio API
        // This avoids the click track from Puppeteer's fake audio device
        const silentAudioContext = new AudioContext();
        const oscillator = silentAudioContext.createOscillator();
        const gainNode = silentAudioContext.createGain();
        gainNode.gain.value = 0; // Silent
        oscillator.connect(gainNode);
        const silentDestination = silentAudioContext.createMediaStreamDestination();
        gainNode.connect(silentDestination);
        oscillator.start();

        localStream = silentDestination.stream;
        console.log('[Recorder] Created silent local stream for WebRTC negotiation');

        updateStatus('Connecting to signaling server...');
        await connectToCall();

      } catch (err) {
        updateStatus('Error: ' + err.message);
        console.error('[Recorder] Init error:', err);
      }
    }

    async function connectToCall() {
      const maxRetries = 60; // Wait up to 60 seconds for call to become active
      const retryDelay = 1000; // 1 second between retries
      let retries = 0;

      while (retries < maxRetries) {
        try {
          // Get call participants via API
          const endpoint = CALL_TYPE === 'video' ? 'video-calls' : 'phone-calls';
          const response = await fetch(`${API_URL}/groups/${GROUP_ID}/${endpoint}`, {
            headers: { 'Authorization': `Bearer ${AUTH_TOKEN}` }
          });

          if (!response.ok) throw new Error('Failed to fetch call info');

          const data = await response.json();
          const calls = CALL_TYPE === 'video' ? data.videoCalls : data.phoneCalls;
          const call = calls?.find(c => c.callId === CALL_ID);

          if (!call) throw new Error('Call not found');

          if (call.status === 'ended') {
            updateStatus('Call ended before recording could start');
            console.log('[Recorder] Call ended, stopping');
            return;
          }

          if (call.status === 'active') {
            updateStatus('Connected. Starting recording...', 'connected');
            logEvent('Recorder initialized, waiting for participants...', 'info');

            // Store participant names for display
            if (call.initiator) {
              participantNames[call.initiator.groupMemberId] = call.initiator.displayName;
            }
            if (call.participants) {
              call.participants.forEach(p => {
                participantNames[p.groupMemberId] = p.displayName;
              });
            }
            participantNames['recorder'] = 'Recording Bot';

            // Set up audio context for mixing streams
            await setupAudioMixer();

            // Poll for WebRTC offers via signaling
            pollForSignals();

            // Recording will start automatically when first track is received
            console.log('[Recorder] Waiting for audio tracks...');
            return;
          }

          // Call not active yet, wait and retry
          if (retries === 0) {
            updateStatus('Waiting for call to start...');
            console.log('[Recorder] Call status:', call.status, '- waiting for active...');
          }

          retries++;
          await new Promise(resolve => setTimeout(resolve, retryDelay));

        } catch (err) {
          updateStatus('Connection error: ' + err.message);
          console.error('[Recorder] Connect error:', err);
          return;
        }
      }

      updateStatus('Timeout waiting for call to become active');
      console.error('[Recorder] Timeout: call never became active');
    }

    let tracksReceived = 0;
    let recordingStarted = false;
    let audioElements = {}; // Store audio elements for each peer
    let gainNodes = {}; // Store gain nodes for volume control

    async function setupAudioMixer() {
      audioContext = new AudioContext({ sampleRate: 48000 });
      // Resume AudioContext (required in headless browsers)
      if (audioContext.state === 'suspended') {
        console.log('[Recorder] Resuming suspended AudioContext...');
        await audioContext.resume();
        console.log('[Recorder] AudioContext state:', audioContext.state);
      }
      mixedDestination = audioContext.createMediaStreamDestination();

      // Also connect to the main destination to keep audio graph active
      // Create a silent gain node connected to speakers (won't make sound)
      const silentGain = audioContext.createGain();
      silentGain.gain.value = 0; // Silent
      silentGain.connect(audioContext.destination);

      console.log('[Recorder] Audio mixer ready, sample rate:', audioContext.sampleRate);
    }

    function addStreamToMixer(stream, label) {
      if (!audioContext || !mixedDestination) {
        console.error('[Recorder] Audio mixer not ready!');
        return;
      }

      try {
        const audioTracks = stream.getAudioTracks();
        console.log('[Recorder] Stream', label, 'has', audioTracks.length, 'audio tracks');

        // Log participant joining with name if available
        const displayName = participantNames[label] || `Participant`;
        logEvent(`${displayName} joined the call`, 'info');

        if (audioTracks.length > 0) {
          const track = audioTracks[0];
          console.log('[Recorder] Track details:', {
            id: track.id,
            kind: track.kind,
            label: track.label,
            enabled: track.enabled,
            muted: track.muted,
            readyState: track.readyState
          });

          // CRITICAL: Create an audio element to play the stream
          // This forces Chrome to actually decode and process the WebRTC audio
          const audioEl = document.createElement('audio');
          audioEl.srcObject = stream;
          audioEl.autoplay = true;
          audioEl.muted = false; // Must NOT be muted for audio to flow
          audioEl.volume = 0.001; // Very low but not zero (zero might stop processing)
          document.getElementById('remoteAudios').appendChild(audioEl);
          audioElements[label] = audioEl;

          // Play the audio element (required for autoplay policy)
          audioEl.play().then(() => {
            console.log('[Recorder] Audio element playing for', label);
          }).catch(err => {
            console.error('[Recorder] Audio element play failed:', err);
          });

          // Create the Web Audio graph
          const source = audioContext.createMediaStreamSource(stream);

          // Add a gain node for volume control and to ensure audio flows
          const gainNode = audioContext.createGain();
          gainNode.gain.value = 1.0; // Full volume to mixer
          gainNodes[label] = gainNode;

          source.connect(gainNode);
          gainNode.connect(mixedDestination);

          // Also connect to destination via silent gain to keep graph active
          const monitorGain = audioContext.createGain();
          monitorGain.gain.value = 0; // Silent monitoring
          gainNode.connect(monitorGain);
          monitorGain.connect(audioContext.destination);

          tracksReceived++;
          console.log('[Recorder] Added stream to mixer:', label, '- Total tracks:', tracksReceived);

          // Monitor track for mute/unmute
          track.onmute = () => console.log('[Recorder] Track muted:', label);
          track.onunmute = () => console.log('[Recorder] Track unmuted:', label);
          track.onended = () => console.log('[Recorder] Track ended:', label);

          // Start recording after first track is received
          if (!recordingStarted && tracksReceived >= 1) {
            console.log('[Recorder] First track received, starting recording...');
            startRecording();
          }
        }
      } catch (err) {
        console.error('[Recorder] Failed to add stream to mixer:', err);
      }
    }

    async function pollForSignals() {
      // Poll the signaling endpoint for WebRTC signals (using recorder-specific endpoint)
      const signalUrl = `${API_URL}/groups/${GROUP_ID}/${CALL_TYPE === 'video' ? 'video-calls' : 'phone-calls'}/${CALL_ID}/recorder-signal`;
      console.log('[Recorder] Signal polling URL:', signalUrl);

      const pollInterval = setInterval(async () => {
        try {
          const response = await fetch(signalUrl, {
            headers: { 'Authorization': `Bearer ${AUTH_TOKEN}` }
          });

          if (!response.ok) {
            console.error('[Recorder] Signal poll failed:', response.status, response.statusText);
            return;
          }

          const data = await response.json();
          if (data.signals && data.signals.length > 0) {
            console.log('[Recorder] Received', data.signals.length, 'signals');
            for (const signal of data.signals) {
              console.log('[Recorder] Processing signal:', signal.type, 'from', signal.from);
              await handleSignal(signal);
            }
          }
        } catch (err) {
          console.error('[Recorder] Signal poll error:', err.message || err);
        }
      }, 2000);

      // Store for cleanup
      window.signalPollInterval = pollInterval;
    }

    async function handleSignal(signal) {
      const { from: fromId, type, data } = signal;
      console.log('[Recorder] Handling signal:', type, 'from peer:', fromId);

      // For ICE candidates, check if we have a peer connection with remote description
      if (type === 'ice-candidate') {
        const pc = peerConnections[fromId];
        if (!pc || !pc.remoteDescription) {
          // Queue the ICE candidate until we have the offer
          if (!pendingIceCandidates[fromId]) {
            pendingIceCandidates[fromId] = [];
          }
          console.log('[Recorder] Queuing ICE candidate for', fromId, '(waiting for offer)');
          pendingIceCandidates[fromId].push(data);
          return;
        }
        // We have remote description, add ICE candidate
        try {
          await pc.addIceCandidate(new RTCIceCandidate(data));
        } catch (err) {
          console.error('[Recorder] Failed to add ICE candidate:', err.message);
        }
        return;
      }

      // Create peer connection if needed (for offer/answer)
      if (!peerConnections[fromId]) {
        console.log('[Recorder] Creating peer connection for', fromId);
        const pc = new RTCPeerConnection({
          iceServers: [
            { urls: 'stun:stun.l.google.com:19302' },
            { urls: 'stun:stun1.l.google.com:19302' }
          ]
        });

        // Add local tracks to peer connection (required for proper WebRTC negotiation)
        // Even though we're not sending real audio, we need to add tracks
        if (localStream) {
          localStream.getTracks().forEach(track => {
            console.log('[Recorder] Adding local track to peer connection:', track.kind);
            pc.addTrack(track, localStream);
          });
        }

        pc.ontrack = (event) => {
          console.log('[Recorder] Received track from', fromId, '- kind:', event.track.kind);
          console.log('[Recorder] Track state:', {
            enabled: event.track.enabled,
            muted: event.track.muted,
            readyState: event.track.readyState
          });
          console.log('[Recorder] Stream has', event.streams[0].getAudioTracks().length, 'audio tracks');
          addStreamToMixer(event.streams[0], fromId);
        };

        pc.onicecandidate = async (event) => {
          if (event.candidate) {
            await sendSignal(fromId, 'ice-candidate', event.candidate);
          }
        };

        pc.onconnectionstatechange = () => {
          console.log('[Recorder] Connection state for', fromId, ':', pc.connectionState);
        };

        pc.oniceconnectionstatechange = () => {
          console.log('[Recorder] ICE connection state for', fromId, ':', pc.iceConnectionState);
        };

        pc.onicegatheringstatechange = () => {
          console.log('[Recorder] ICE gathering state for', fromId, ':', pc.iceGatheringState);
        };

        peerConnections[fromId] = pc;
      }

      const pc = peerConnections[fromId];

      if (type === 'offer') {
        console.log('[Recorder] Processing offer from', fromId);
        await pc.setRemoteDescription(new RTCSessionDescription(data));
        const answer = await pc.createAnswer();
        await pc.setLocalDescription(answer);
        await sendSignal(fromId, 'answer', answer);
        console.log('[Recorder] Sent answer to', fromId);

        // Process any queued ICE candidates
        if (pendingIceCandidates[fromId] && pendingIceCandidates[fromId].length > 0) {
          console.log('[Recorder] Processing', pendingIceCandidates[fromId].length, 'queued ICE candidates for', fromId);
          for (const candidate of pendingIceCandidates[fromId]) {
            try {
              await pc.addIceCandidate(new RTCIceCandidate(candidate));
            } catch (err) {
              console.error('[Recorder] Failed to add queued ICE candidate:', err.message);
            }
          }
          delete pendingIceCandidates[fromId];
        }
      } else if (type === 'answer') {
        await pc.setRemoteDescription(new RTCSessionDescription(data));
      }
    }

    async function sendSignal(toId, type, data) {
      try {
        const endpoint = CALL_TYPE === 'video' ? 'video-calls' : 'phone-calls';
        await fetch(`${API_URL}/groups/${GROUP_ID}/${endpoint}/${CALL_ID}/recorder-signal`, {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${AUTH_TOKEN}`,
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            targetPeerId: toId,
            type,
            data
          })
        });
      } catch (err) {
        console.error('[Recorder] Send signal error:', err);
      }
    }

    function startRecording() {
      if (isRecording || recordingStarted || !mixedDestination) return;
      recordingStarted = true;

      try {
        const stream = mixedDestination.stream;
        const tracks = stream.getAudioTracks();
        console.log('[Recorder] Mixed stream tracks:', tracks.length);

        if (tracks.length > 0) {
          const mixedTrack = tracks[0];
          console.log('[Recorder] Mixed track details:', {
            id: mixedTrack.id,
            enabled: mixedTrack.enabled,
            muted: mixedTrack.muted,
            readyState: mixedTrack.readyState
          });
        }

        isRecording = true;
        updateStatus('Recording in progress...', 'recording');

        // Start first chunk
        startNewChunk();

        // Schedule chunk rotation
        scheduleChunkRotation();

        // Expose stop function for Puppeteer
        window.stopRecording = stopRecording;
        window.isRecording = true;

        // Log recording stats periodically
        const statsInterval = setInterval(() => {
          if (!isRecording) {
            clearInterval(statsInterval);
            return;
          }
          console.log('[Recorder] Recording stats - active recorders:', activeRecorders.length,
                      ', chunks uploaded:', currentChunkIndex - activeRecorders.length);
        }, 5000);
        window.statsInterval = statsInterval;

      } catch (err) {
        console.error('[Recorder] Start recording error:', err);
        updateStatus('Recording failed: ' + err.message);
      }
    }

    function startNewChunk() {
      const stream = mixedDestination.stream;
      const chunkIndex = currentChunkIndex++;

      // Use webm/opus codec for better compatibility
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
        ? 'audio/webm;codecs=opus'
        : 'audio/webm';

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: mimeType,
        audioBitsPerSecond: 128000 // 128kbps for good quality
      });

      const recorderState = {
        mediaRecorder,
        chunks: [],
        chunkIndex,
        startedAt: new Date()
      };

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          recorderState.chunks.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        recorderState.endedAt = new Date();
        console.log(`[Recorder] Chunk ${chunkIndex} stopped, data size:`,
                    recorderState.chunks.reduce((sum, c) => sum + c.size, 0), 'bytes');

        // Remove from active recorders
        activeRecorders = activeRecorders.filter(r => r.chunkIndex !== chunkIndex);

        // Queue for upload
        queueChunkForUpload(recorderState);
      };

      mediaRecorder.onerror = (event) => {
        console.error(`[Recorder] Chunk ${chunkIndex} error:`, event.error);
      };

      mediaRecorder.start(1000); // Collect data every second
      activeRecorders.push(recorderState);

      console.log(`[Recorder] Started chunk ${chunkIndex}, active recorders:`, activeRecorders.length);
      logEvent(`Recording ${chunkIndex + 1} STARTED`, 'info');
    }

    function scheduleChunkRotation() {
      // Start new chunk before stopping old one (overlap period)
      const rotationTime = CHUNK_DURATION_MS - OVERLAP_DURATION_MS;

      chunkTimer = setInterval(() => {
        if (!isRecording) {
          clearInterval(chunkTimer);
          return;
        }

        console.log('[Recorder] Rotating chunks - starting new chunk first (gapless)');

        // Start new chunk FIRST (gapless - overlap period begins)
        startNewChunk();

        // Stop the old chunk after overlap period
        setTimeout(() => {
          const oldRecorder = activeRecorders.find(r => r.chunkIndex === currentChunkIndex - 2);
          if (oldRecorder && oldRecorder.mediaRecorder.state === 'recording') {
            console.log(`[Recorder] Stopping old chunk ${oldRecorder.chunkIndex} after overlap`);
            oldRecorder.mediaRecorder.stop();
          }
        }, OVERLAP_DURATION_MS);
      }, rotationTime);
    }

    function queueChunkForUpload(recorderState) {
      uploadQueue.push(recorderState);
      processUploadQueue();
    }

    async function processUploadQueue() {
      if (isUploading || uploadQueue.length === 0) return;

      isUploading = true;
      const chunkData = uploadQueue.shift();

      try {
        await uploadChunk(chunkData);
      } catch (err) {
        console.error('[Recorder] Chunk upload failed:', err);
        // Re-queue failed chunk at the beginning
        uploadQueue.unshift(chunkData);
      }

      isUploading = false;
      processUploadQueue();
    }

    async function uploadChunk(recorderState) {
      const { chunks, chunkIndex, startedAt, endedAt } = recorderState;

      if (chunks.length === 0) {
        console.log(`[Recorder] Chunk ${chunkIndex} has no data, skipping upload`);
        return;
      }

      const blob = new Blob(chunks, { type: 'audio/webm' });
      const sizeMB = (blob.size / (1024 * 1024)).toFixed(2);

      logEvent(`Recording ${chunkIndex + 1} ENDED (${sizeMB} MB)`, 'info');
      logEvent(`Recording ${chunkIndex + 1} uploading to S3...`, 'info');

      const formData = new FormData();
      formData.append('recording', blob, `chunk-${chunkIndex}.webm`);
      formData.append('chunkIndex', chunkIndex.toString());
      formData.append('startedAt', startedAt.toISOString());
      formData.append('endedAt', (endedAt || new Date()).toISOString());

      console.log(`[Recorder] Uploading chunk ${chunkIndex}, size:`, blob.size, 'bytes');

      const endpoint = CALL_TYPE === 'video' ? 'video-calls' : 'phone-calls';
      const response = await fetch(
        `${API_URL}/groups/${GROUP_ID}/${endpoint}/${CALL_ID}/recording-chunk`,
        {
          method: 'POST',
          headers: { 'Authorization': `Bearer ${AUTH_TOKEN}` },
          body: formData
        }
      );

      if (response.ok) {
        const data = await response.json();
        console.log(`[Recorder] Chunk ${chunkIndex} uploaded successfully:`, data);
        logEvent(`Recording ${chunkIndex + 1} UPLOADED to S3 âœ“`, 'info');
      } else {
        const errorText = await response.text();
        logEvent(`Recording ${chunkIndex + 1} upload FAILED`, 'error');
        throw new Error(`Upload failed: ${response.status} - ${errorText}`);
      }
    }

    function stopRecording() {
      return new Promise(async (resolve) => {
        if (!isRecording) {
          resolve();
          return;
        }

        const activeCount = activeRecorders.filter(r => r.mediaRecorder.state === 'recording').length;
        const chunkNumbers = activeRecorders
          .filter(r => r.mediaRecorder.state === 'recording')
          .map(r => r.chunkIndex + 1)
          .join(', ');

        logEvent(`CALL ENDED - saving ${activeCount} recording(s)`, 'warning');

        isRecording = false;
        window.isRecording = false;
        updateStatus('Recording stopped, uploading remaining chunks...');

        // Broadcast which chunks are being uploaded BEFORE stopping
        if (activeCount > 0) {
          logEvent(`Uploading Recording ${chunkNumbers} to S3 (this may take a moment)...`, 'info');
        }

        // Clear chunk rotation timer
        if (chunkTimer) {
          clearInterval(chunkTimer);
          chunkTimer = null;
        }

        // Stop all active recorders
        const stopPromises = activeRecorders.map(r => {
          return new Promise((res) => {
            if (r.mediaRecorder.state === 'recording') {
              const originalOnStop = r.mediaRecorder.onstop;
              r.mediaRecorder.onstop = () => {
                if (originalOnStop) originalOnStop();
                res();
              };
              r.mediaRecorder.stop();
            } else {
              res();
            }
          });
        });

        await Promise.all(stopPromises);

        // Wait for upload queue to be processed
        const waitForUploads = () => {
          return new Promise((res) => {
            const check = () => {
              if (uploadQueue.length === 0 && !isUploading) {
                res();
              } else {
                setTimeout(check, 500);
              }
            };
            check();
          });
        };

        await waitForUploads();

        updateStatus('All chunks uploaded successfully!', 'connected');
        console.log('[Recorder] All recordings uploaded');
        resolve();
      });
    }

    function cleanup() {
      console.log('[Recorder] Cleaning up...');

      if (window.signalPollInterval) {
        clearInterval(window.signalPollInterval);
      }

      if (window.statsInterval) {
        clearInterval(window.statsInterval);
      }

      if (chunkTimer) {
        clearInterval(chunkTimer);
        chunkTimer = null;
      }

      // Stop and remove audio elements
      Object.entries(audioElements).forEach(([label, el]) => {
        console.log('[Recorder] Stopping audio element for', label);
        el.pause();
        el.srcObject = null;
        el.remove();
      });
      audioElements = {};

      // Stop all active recorders
      activeRecorders.forEach(r => {
        if (r.mediaRecorder.state === 'recording') {
          r.mediaRecorder.stop();
        }
      });
      activeRecorders = [];

      Object.values(peerConnections).forEach(pc => pc.close());
      peerConnections = {};

      if (localStream) {
        localStream.getTracks().forEach(track => track.stop());
      }

      if (audioContext && audioContext.state !== 'closed') {
        audioContext.close();
      }

      console.log('[Recorder] Cleanup complete');
    }

    // Expose for Puppeteer control
    window.recorderReady = false;
    window.cleanup = cleanup;

    // Initialize
    init().then(() => {
      window.recorderReady = true;
    });
  </script>
</body>
</html>
